{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DUMMY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://github.com/unslothai/unsloth\n",
    "* https://docs.unsloth.ai/get-started/installing-+-updating/windows-installation  METHOD#2 (CC,CXX, CL path)\n",
    "\n",
    "* conda deactivate\n",
    "* conda remove --name env_llm_qlora --all -y\n",
    "\n",
    "Install cudatoolkit\n",
    "* conda create --name env_llm_qlora python=3.11 pytorch-cuda=12.1 pytorch cudatoolkit -c pytorch -c nvidia -y\n",
    "* conda activate env_llm_qlora\n",
    "\n",
    "\n",
    "* ####python -m pip install xformers | conda install -c conda-forge xformers |\n",
    "\n",
    "* python -m pip install -U xformers --index-url https://download.pytorch.org/whl/cu121\n",
    "* [Blackwell] python -m pip install -U xformers --index-url https://download.pytorch.org/whl/nightly/cu128 (python -m pip uninstall xformers -y)\n",
    "\n",
    "python -m pip install \"unsloth[windows] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "python -m pip install --no-deps trl peft accelerate bitsandbytes\n",
    "python -m pip install ipywidgets ipykernel\n",
    "python -m pip install jmespath\n",
    "python -m pip install --upgrade accelerate\n",
    "python -m pip install tensorboard\n",
    "\n",
    "[Blackwell]\n",
    "python -m pip uninstall torch torchvision torchaudio -y\n",
    "python -m pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128\n",
    "python -m pip install --no-deps xformers==0.0.29.post3\n",
    "\n",
    "Patch: num_proc = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import json\n",
    "\n",
    "# Unsloth always first as import, otherwise will create some inconsistent errors\n",
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "from unsloth.chat_templates import get_chat_template, train_on_responses_only\n",
    "\n",
    "from trl import SFTTrainer\n",
    "from datasets import Dataset\n",
    "from threading import Thread\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq, TextIteratorStreamer, TextStreamer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA disponible: True\n",
      "Dispositivo actual: NVIDIA GeForce RTX 5070 Ti\n",
      "Cantidad de GPUs disponibles: 2\n"
     ]
    }
   ],
   "source": [
    "cuda_available = torch.cuda.is_available()\n",
    "print(f\"CUDA disponible: {cuda_available}\")\n",
    "\n",
    "# Mostrar el dispositivo actual\n",
    "if cuda_available:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Dispositivo actual: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Cantidad de GPUs disponibles: {torch.cuda.device_count()}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Se est√° utilizando la CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cargando el modelo base y el tokenizador...\n",
      "==((====))==  Unsloth 2025.4.7: Fast Llama patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 5070 Ti. Num GPUs = 2. Max memory: 15.92 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu128. CUDA: 12.0. CUDA Toolkit: 12.8. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a689d9fb6584412a6725c7565a0a020",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d44816ef1e4f4e768b048a07953ea946",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/230 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f99f0394983416e93dd9c5cd8ed38ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/50.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31553f85d1854755b5e7270d2d0d7c4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "069d3eb135364912b1f0d8066f830fea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/459 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo y tokenizador cargados con √©xito.\n",
      "\n",
      "Aplicando configuraci√≥n LoRA al modelo...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Not an error, but Unsloth cannot patch MLP layers with our manual autograd engine since either LoRA adapters\n",
      "are not enabled or a bias term (like in Qwen) is used.\n",
      "Unsloth 2025.4.7 patched 28 layers with 28 QKV layers, 28 O layers and 0 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuraci√≥n LoRA aplicada.\n"
     ]
    }
   ],
   "source": [
    "# Definimos par√°metros importantes\n",
    "max_seq_length = 512         # M√°xima longitud de secuencia (aj√∫stala si necesitas m√°s contexto)\n",
    "dtype = None                 # Puede ser especificado, en este ejemplo lo dejamos por defecto\n",
    "load_in_4bit = True          # Usamos 4bit para optimizar uso de VRAM\n",
    "\n",
    "print(\"\\nCargando el modelo base y el tokenizador...\")\n",
    "# Cargamos el modelo preentrenado de Unsloth. Se utilizar√° el modelo LLaMA-3.2-3B-bnb-4bit.\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/Llama-3.2-3B-bnb-4bit\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")\n",
    "print(\"Modelo y tokenizador cargados con √©xito.\")\n",
    "\n",
    "# Configuramos el modelo para LoRA. Se inyectan matrices LoRA en las proyecciones relevantes.\n",
    "print(\"\\nAplicando configuraci√≥n LoRA al modelo...\")\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,  # Puede ajustarse; valores sugeridos: 8, 16, 32, 64, etc.\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # , \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,   # Se recomienda 0 para optimizaci√≥n\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",  # Para manejar contextos largos\n",
    "    random_state=4242,\n",
    "    use_rslora=False,  # Desactivar RSLORA si no se requiere\n",
    "    loftq_config=None,\n",
    ")\n",
    "print(\"Configuraci√≥n LoRA aplicada.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| M√≥dulo           | ¬øCu√°ndo incluirlo en `target_modules`?                                             | Notas adicionales |\n",
    "|------------------|------------------------------------------------------------------------------------|-------------------|\n",
    "| `q_proj`         | ‚úÖ **Siempre**                                                                     | La matriz Query determina c√≥mo se consulta la atenci√≥n, esencial para aprendizaje de contextos espec√≠ficos y conversaciones. |\n",
    "| `k_proj`         | ‚úÖ **Siempre**                                                                     | La matriz Key determina c√≥mo se organizan las representaciones internas del contexto. Necesaria para capturar patrones sem√°nticos. |\n",
    "| `v_proj`         | ‚úÖ **Siempre**                                                                     | La matriz Value representa el valor real o contenido sem√°ntico; fundamental para generar respuestas coherentes y precisas. |\n",
    "| `o_proj`         | ‚úÖ **Siempre o casi siempre**                                                       | Proyecci√≥n de salida final en atenci√≥n; afecta directamente la calidad y claridad de las respuestas. Suele mejorar la coherencia. |\n",
    "| `gate_proj`      | üî∂ **A veces** (√ötil especialmente en tareas complejas o textos largos)            | Controla la activaci√≥n en capas FFN; ayuda en tareas con mucha informaci√≥n o patrones complejos. Puede mejorar calidad, pero a√±ade memoria y tiempo. |\n",
    "| `up_proj`        | üî∂ **A veces** (√ötil en datasets grandes o muy diversos)                           | Expande la dimensi√≥n interna en FFN. Agregarla mejora ligeramente capacidad expresiva pero consume m√°s memoria. |\n",
    "| `down_proj`      | üî∂ **A veces** (√ötil junto con `up_proj` en tareas complejas o datasets variados)  | Reduce la dimensi√≥n en FFN, act√∫a junto con `up_proj`. Se recomienda si se usa `up_proj`. |\n",
    "\n",
    "### üìå **Recomendaciones pr√°cticas r√°pidas:**\n",
    "\n",
    "- **Casos est√°ndar (p. ej. chatbots, asistentes):**\n",
    "  ```python\n",
    "  target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    "  ```\n",
    "\n",
    "- **Casos avanzados (textos largos, tareas complejas, fine-tuning intensivo):**\n",
    "  ```python\n",
    "  target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuraci√≥n de tokens especiales completada.\n"
     ]
    }
   ],
   "source": [
    "# Si el tokenizador no tiene definido un token de padding, lo asignamos a uno de los tokens reservados.\n",
    "if tokenizer.pad_token is None:\n",
    "    # Elegimos usar <|reserved_special_token_0|> como token de padding\n",
    "    tokenizer.pad_token = \"<|reserved_special_token_0|>\"\n",
    "    print(\"Se asign√≥ <|reserved_special_token_0|> como token PAD.\")\n",
    "\n",
    "# Verificamos que el modelo tenga configurado el pad_token_id.\n",
    "if model.config.pad_token_id is None:\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    print(f\"Se configur√≥ el pad_token_id del modelo a: {model.config.pad_token_id}\")\n",
    "\n",
    "# Asegurarse de que el token EOS est√© configurado (por lo general es <|end_of_text|>)\n",
    "if not hasattr(tokenizer, \"eos_token\") or tokenizer.eos_token is None:\n",
    "    # En muchos modelos, el EOS es \"<|end_of_text|>\"; ajusta seg√∫n tu modelo.\n",
    "    tokenizer.eos_token = \"<|end_of_text|>\"\n",
    "    print(\"Se asign√≥ <|end_of_text|> como token EOS.\")\n",
    "\n",
    "print(\"Configuraci√≥n de tokens especiales completada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|finetune_right_pad_id|>\n",
      "<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.pad_token)\n",
    "print(tokenizer.eos_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You typically don't have to manually add <|end_of_text|> in the conversation formatting or the dataset itself. The token <|end_of_text|> is added automatically by the tokenizer or the generation logic to indicate the end of generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Leyendo y procesando el dataset de conversaciones...\n",
      "Total de ejemplos cargados: 241\n",
      "Sample: {'conversation': [{'role': 'system', 'content': 'Eres una streamer divertida, carism√°tica y aut√©ntica. Siempre respondes con humor, espontaneidad y energ√≠a positiva. Interact√∫as de forma cercana con tu audiencia, utilizando expresiones coloquiales y reacciones genuinas.'}, {'content': '¬øQu√© tipo de gaseosa te gusta?', 'role': 'user'}, {'content': 'Gaseosa de naranja, obviamente. Es como el jugo de la vida pero con burbujas.', 'role': 'assistant'}]}\n",
      "Dataset creado correctamente.\n",
      "\n",
      "Ejemplo de conversaci√≥n original (sin formatear):\n",
      "[{'content': 'Eres una streamer divertida, carism√°tica y aut√©ntica. Siempre respondes con humor, espontaneidad y energ√≠a positiva. Interact√∫as de forma cercana con tu audiencia, utilizando expresiones coloquiales y reacciones genuinas.', 'role': 'system'}, {'content': '¬øQu√© tipo de gaseosa te gusta?', 'role': 'user'}, {'content': 'Gaseosa de naranja, obviamente. Es como el jugo de la vida pero con burbujas.', 'role': 'assistant'}]\n"
     ]
    }
   ],
   "source": [
    "# Definimos el system prompt para fijar la personalidad del asistente.\n",
    "system_prompt = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": (\"Eres una streamer divertida, carism√°tica y aut√©ntica. \"\n",
    "                \"Siempre respondes con humor, espontaneidad y energ√≠a positiva. \"\n",
    "                \"Interact√∫as de forma cercana con tu audiencia, utilizando expresiones coloquiales y reacciones genuinas.\")\n",
    "}\n",
    "\n",
    "# Ruta del dataset (formato JSONL, donde cada l√≠nea es una conversaci√≥n)\n",
    "dataset_path = \"./temp/dataset.jsonl\"\n",
    "\n",
    "print(\"\\nLeyendo y procesando el dataset de conversaciones...\")\n",
    "data = []\n",
    "# Abrimos el archivo y procesamos cada l√≠nea\n",
    "with open(dataset_path, \"r\", encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        conversation = json.loads(line)\n",
    "        # Verificamos que la conversaci√≥n sea una lista y que cada mensaje tenga 'role' y 'content'\n",
    "        if isinstance(conversation, list):\n",
    "            # Insertamos el system prompt al inicio para anclar la personalidad\n",
    "            conversation.insert(0, system_prompt)\n",
    "            # Realizamos una comprobaci√≥n b√°sica: cada mensaje debe ser un dict con 'role' y 'content'\n",
    "            valid = all(isinstance(m, dict) and \"role\" in m and \"content\" in m for m in conversation)\n",
    "            assert all(\"role\" in m and \"content\" in m for m in conversation), \"Conversaci√≥n mal formada.\"\n",
    "        \n",
    "            if not valid:\n",
    "                print(\"Advertencia: Se encontr√≥ una conversaci√≥n con formato inesperado. Revisar:\")\n",
    "                print(conversation)\n",
    "            data.append({\"conversation\": conversation})\n",
    "        else:\n",
    "            print(\"L√≠nea con formato inesperado:\", line)\n",
    "\n",
    "print(f\"Total de ejemplos cargados: {len(data)}\")\n",
    "print(f\"Sample: {data[0]}\")\n",
    "\n",
    "# Creamos el objeto dataset de Hugging Face\n",
    "dataset = Dataset.from_list(data)\n",
    "print(\"Dataset creado correctamente.\")\n",
    "\n",
    "# Imprimir un ejemplo del dataset original para validaci√≥n\n",
    "print(\"\\nEjemplo de conversaci√≥n original (sin formatear):\")\n",
    "print(dataset[0][\"conversation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurando la plantilla de chat para el tokenizador...\n",
      "Plantilla de chat aplicada.\n",
      "\n",
      "Ejemplo de conversaci√≥n (raw):\n",
      "[{'content': 'Eres una streamer divertida, carism√°tica y aut√©ntica. Siempre respondes con humor, espontaneidad y energ√≠a positiva. Interact√∫as de forma cercana con tu audiencia, utilizando expresiones coloquiales y reacciones genuinas.', 'role': 'system'}, {'content': '¬øQu√© tipo de gaseosa te gusta?', 'role': 'user'}, {'content': 'Gaseosa de naranja, obviamente. Es como el jugo de la vida pero con burbujas.', 'role': 'assistant'}]\n",
      "\n",
      "Ejemplo de conversaci√≥n formateada:\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 July 2024\n",
      "\n",
      "Eres una streamer divertida, carism√°tica y aut√©ntica. Siempre respondes con humor, espontaneidad y energ√≠a positiva. Interact√∫as de forma cercana con tu audiencia, utilizando expresiones coloquiales y reacciones genuinas.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "¬øQu√© tipo de gaseosa te gusta?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Gaseosa de naranja, obviamente. Es como el jugo de la vida pero con burbujas.<|eot_id|>\n",
      "\n",
      "Aplicando formateo a todo el dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cadf405d43b4318a3d9a88e7830a705",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/241 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formateo aplicado. Mostrando ejemplo final del dataset:\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 July 2024\n",
      "\n",
      "Eres una streamer divertida, carism√°tica y aut√©ntica. Siempre respondes con humor, espontaneidad y energ√≠a positiva. Interact√∫as de forma cercana con tu audiencia, utilizando expresiones coloquiales y reacciones genuinas.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "¬øQu√© tipo de gaseosa te gusta?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Gaseosa de naranja, obviamente. Es como el jugo de la vida pero con burbujas.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nConfigurando la plantilla de chat para el tokenizador...\")\n",
    "# Inicializamos el tokenizador con la plantilla \"llama-3.2\"\n",
    "tokenizer = get_chat_template(tokenizer, chat_template=\"llama-3.2\")\n",
    "print(\"Plantilla de chat aplicada.\")\n",
    "\n",
    "# Funci√≥n para formatear cada conversaci√≥n utilizando la plantilla\n",
    "def formatting_prompts_func(examples):\n",
    "    texts = []\n",
    "    for convo in examples[\"conversation\"]:\n",
    "        # Aplicamos la plantilla de chat a cada conversaci√≥n\n",
    "        formatted = tokenizer.apply_chat_template(\n",
    "            convo,\n",
    "            tokenize=False,            # Queremos el texto legible para depuraci√≥n\n",
    "            add_generation_prompt=False  # No agregamos prompt adicional en esta etapa\n",
    "        )\n",
    "        texts.append(formatted)\n",
    "    # Se devuelve un diccionario con la clave \"text\" para el dataset\n",
    "    return {\"text\": texts}\n",
    "\n",
    "# Verificamos manualmente el formateo en un ejemplo espec√≠fico\n",
    "example_index = 0  # Cambia el √≠ndice seg√∫n convenga\n",
    "example_convo = dataset[example_index][\"conversation\"]\n",
    "print(\"\\nEjemplo de conversaci√≥n (raw):\")\n",
    "print(example_convo)\n",
    "\n",
    "# Aplicamos la plantilla al ejemplo y mostramos el resultado\n",
    "formatted_example = tokenizer.apply_chat_template(example_convo, tokenize=False, add_generation_prompt=False)\n",
    "print(\"\\nEjemplo de conversaci√≥n formateada:\")\n",
    "print(formatted_example)\n",
    "\n",
    "# Mapeamos el dataset completo para aplicar el formateo\n",
    "print(\"\\nAplicando formateo a todo el dataset...\")\n",
    "dataset = dataset.map(formatting_prompts_func, batched=True)\n",
    "print(\"Formateo aplicado. Mostrando ejemplo final del dataset:\")\n",
    "print(dataset[example_index][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The token <|eot_id|> is a special marker defined explicitly by the chat template (llama-3.2) to separate segments of conversation internally. It represents the end of turn (End-Of-Turn) in conversations, which can be the end of system, user, or assistant messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 6. Definir funciones de generaci√≥n para pruebas (One-shot y Streaming)\n",
    "# ---------------------------------------------------------------------------\n",
    "def response_normal(model, tokenizer, messages, temperature=0.7, max_new_tokens=64, skip_special_tokens=False):\n",
    "    \"\"\"\n",
    "    Funci√≥n para obtener la respuesta completa (one-shot) sin streaming.\n",
    "    Se habilita la inferencia optimizada y se generan tokens.\n",
    "    \"\"\"\n",
    "    # Optimizaci√≥n de inferencia con Unsloth\n",
    "    FastLanguageModel.for_inference(model)\n",
    "    \n",
    "    # Preparar las entradas usando la plantilla de chat\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,  # Es necesario para la generaci√≥n\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(device)\n",
    "    \n",
    "    # Generaci√≥n de respuesta (one-shot)\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        use_cache=True,\n",
    "        temperature=0.7,              # Lower this to reduce randomness\n",
    "        # repetition_penalty=1.1,       # Avoid repetitive outputs\n",
    "        # min_p=0.1\n",
    "    )\n",
    "    \n",
    "    # Decodificar y retornar la respuesta, omitiendo tokens especiales\n",
    "    decoded = tokenizer.batch_decode(outputs, skip_special_tokens=skip_special_tokens)\n",
    "    return decoded\n",
    "\n",
    "# Funci√≥n opcional para generaci√≥n en streaming (comentada en este ejemplo)\n",
    "def response_stream(model, tokenizer, messages):\n",
    "    \"\"\"\n",
    "    Funci√≥n para generaci√≥n de respuesta en streaming.\n",
    "    Permite ver los tokens generados progresivamente.\n",
    "    \"\"\"\n",
    "    FastLanguageModel.for_inference(model)\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(device)\n",
    "    \n",
    "    streamer = TextIteratorStreamer(tokenizer, skip_prompt=True)\n",
    "    thread = Thread(target=model.generate, kwargs={\n",
    "        \"input_ids\": inputs,\n",
    "        \"streamer\": streamer,\n",
    "        \"max_new_tokens\": 128,\n",
    "        \"use_cache\": True,\n",
    "        \"temperature\": 1.0,\n",
    "        \"min_p\": 0.1\n",
    "    })\n",
    "    thread.start()\n",
    "    for new_text in streamer:\n",
    "        print(new_text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# START behaviour\n",
    "# ------------------------------------------------------------------------------\n",
    "# Get first message from a conversation:\n",
    "conversation_user_message = list(filter(lambda x: x['role'] == 'user', dataset[5][\"conversation\"]))[0]['content'] # first message\n",
    "messages = [\n",
    "    system_prompt,\n",
    "    {\"role\": \"user\", \"content\": conversation_user_message}\n",
    "]\n",
    "\n",
    "# Don't run because it will change to inference mode, but if you run remember to restore the model to previous state\n",
    "# print(messages)\n",
    "# # 1) Generaci√≥n normal (sin streaming)\n",
    "# print(\"BY ONE SHOT:\")\n",
    "# llm_response = response_normal(model, tokenizer, messages)\n",
    "# print(llm_response[-1])\n",
    "\n",
    "# 2) Generaci√≥n en streaming\n",
    "# print(\"\\nBY STREAMING:\")\n",
    "# for chunk in response_stream(model, tokenizer, messages):\n",
    "#     print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model hasn't been fine-tuned yet, so it's just responding based on random weights from LoRA adapters.\n",
    "\n",
    "Generation parameters (like temperature or repetition_penalty) aren't correctly configured.\n",
    "\n",
    "To create a comparison of before and afer: Run inference with the original pretrained base model without LoRA adapters. You should see generic but coherent outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # For retrain\n",
    "    # Reload Model & Tokenizer (as above)\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=\"./models/qlora_mirai_v0-3B\",\n",
    "        max_seq_length=512,\n",
    "        load_in_4bit=True\n",
    "    )\n",
    "\n",
    "    model = FastLanguageModel.get_peft_model(\n",
    "        model,\n",
    "        r=16,\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "        lora_alpha=16,\n",
    "        random_state=42,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurando el entrenamiento con QLoRA usando SFTTrainer...\n",
      "Argumentos de entrenamiento configurados:\n",
      "TrainingArguments(\n",
      "_n_gpu=2,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "average_tokens_across_devices=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=None,\n",
      "eval_strategy=no,\n",
      "eval_use_gather_object=False,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=4,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=None,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_for_metrics=[],\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.0001,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./outputs/runs/May05_21-22-46_magod,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=2,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=2,\n",
      "optim=adamw_8bit,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=./outputs,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=2,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./outputs,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=4242,\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tp_size=0,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=10,\n",
      "weight_decay=0.01,\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00fe131fe1ab466c90f3d1fa3ab7f2f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"]:   0%|          | 0/241 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFTTrainer configurado correctamente.\n",
      "\n",
      "Aplicando train_on_responses_only para enmascarar las partes de usuario...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68f941faf3d147f686f2fa219d0eea47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=24):   0%|          | 0/241 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuraci√≥n de enmascaramiento aplicada.\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 7. Configuraci√≥n del Trainer para Fine-Tuning con QLoRA\n",
    "# ---------------------------------------------------------------------------\n",
    "print(\"\\nConfigurando el entrenamiento con QLoRA usando SFTTrainer...\")\n",
    "\n",
    "# Creamos el objeto de entrenamiento con SFTTrainer de TRL (usado por Unsloth)\n",
    "trainer = None\n",
    "\n",
    "# Configuramos los argumentos de entrenamiento con hiperpar√°metros recomendados.\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=2,             # Batch peque√±o debido a la cantidad de par√°metros\n",
    "    gradient_accumulation_steps=4,             # Acumulamos gradientes para simular un batch m√°s grande\n",
    "    warmup_steps=10,                            # Pasos de warmup\n",
    "    num_train_epochs=2,                        # Set this for 1 full training run.\n",
    "    # max_steps=60,                            # N√∫mero total de pasos (ajusta seg√∫n el tama√±o de tu dataset)\n",
    "    learning_rate=1e-4,                        # Tasa de aprendizaje\n",
    "    fp16=not is_bfloat16_supported(),          # Usamos fp16 si no se soporta bf16\n",
    "    bf16=is_bfloat16_supported(),              # bf16 si es soportado por la GPU\n",
    "    logging_steps=2,                          # Registro cada 10 pasos\n",
    "    optim=\"adamw_8bit\",                        # Optimizador eficiente en memoria\n",
    "    weight_decay=0.01,                         # Regularizaci√≥n\n",
    "    lr_scheduler_type=\"linear\",                # Scheduler lineal\n",
    "    seed=4242,                                 # Fijamos la semilla para reproducibilidad\n",
    "    output_dir=\"./outputs\",                    # Directorio de salida para guardar checkpoints\n",
    "    report_to=\"tensorboard\",                          # Deshabilitamos reportes externos (ej. WandB)\n",
    ")\n",
    "\n",
    "print(\"Argumentos de entrenamiento configurados:\")\n",
    "print(training_args)\n",
    "\n",
    "# Creamos el DataCollator para secuencias (ajustado para seq2seq)\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer)\n",
    "\n",
    "# Inicializamos el Trainer con el modelo, tokenizador, dataset, y data collator\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,          # El dataset ya formateado tiene la columna \"text\"\n",
    "    dataset_text_field=\"text\",      # Indicamos que la entrada es la clave \"text\"\n",
    "    max_seq_length=max_seq_length,\n",
    "    data_collator=data_collator,\n",
    "    dataset_num_proc=1,             # N√∫mero de procesos para mapear (ajusta seg√∫n tu CPU)\n",
    "    packing=False,                  # Si las secuencias son cortas, packing puede acelerar el entrenamiento\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "print(\"SFTTrainer configurado correctamente.\")\n",
    "\n",
    "# Aplicamos la funci√≥n train_on_responses_only para entrenar solo en las respuestas del asistente.\n",
    "# Es fundamental que se especifiquen correctamente los delimitadores seg√∫n la plantilla usada.\n",
    "print(\"\\nAplicando train_on_responses_only para enmascarar las partes de usuario...\")\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part=\"<|start_header_id|>user<|end_header_id|>\\n\\n\",  # Delimitador para instrucciones (usuario)\n",
    "    response_part=\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",  # Delimitador para respuestas (asistente)\n",
    ")\n",
    "print(\"Configuraci√≥n de enmascaramiento aplicada.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We verify masking is actually done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\nEres una streamer divertida, carism√°tica y aut√©ntica. Siempre respondes con humor, espontaneidad y energ√≠a positiva. Interact√∫as de forma cercana con tu audiencia, utilizando expresiones coloquiales y reacciones genuinas.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n¬øPrefieres series o pel√≠culas?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nPrefiero series, as√≠ tengo excusa para procrastinar varios d√≠as seguidos.<|eot_id|>'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(trainer.train_dataset[5][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'                                                                                                        Prefiero series, as√≠ tengo excusa para procrastinar varios d√≠as seguidos.<|eot_id|>'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# You should not see the system or user part\n",
    "space = tokenizer(\" \", add_special_tokens = False).input_ids[0]\n",
    "tokenizer.decode([space if x == -100 else x for x in trainer.train_dataset[5][\"labels\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iniciando el entrenamiento...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 241 | Num Epochs = 2 | Total steps = 30\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16\n",
      " \"-____-\"     Trainable parameters = 9,175,040/3,000,000,000 (0.31% trained)\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "print(\"\\nIniciando el entrenamiento...\")\n",
    "trainer_stats = trainer.train()\n",
    "print(\"\\nEntrenamiento completado. Estad√≠sticas:\")\n",
    "print(trainer_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos la ruta para guardar el modelo fine-tuneado\n",
    "output_model_dir = \"./models/qlora_mirai_v0-3B\"\n",
    "os.makedirs(output_model_dir, exist_ok=True)\n",
    "\n",
    "print(\"Guardando el modelo y el tokenizador en:\", output_model_dir)\n",
    "model.save_pretrained(output_model_dir)\n",
    "tokenizer.save_pretrained(output_model_dir)\n",
    "print(\"Modelo y tokenizador guardados correctamente.\")\n",
    "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
    "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    # For new loading\n",
    "    from unsloth import FastLanguageModel\n",
    "    model_inference, tokenizer_inference = FastLanguageModel.from_pretrained(\n",
    "        model_name = output_model_dir, # YOUR MODEL YOU USED FOR TRAINING\n",
    "        max_seq_length = max_seq_length,\n",
    "        dtype = dtype,\n",
    "        load_in_4bit = load_in_4bit,\n",
    "    )\n",
    "    FastLanguageModel.for_inference(model_inference)\n",
    "\n",
    "    test_messages = [\n",
    "        system_prompt,\n",
    "        {\"role\": \"user\", \"content\": \"Rivers, ¬øqu√© opinas del nuevo parche del LoL?\"}\n",
    "    ]\n",
    "\n",
    "    inputs = tokenizer_inference.apply_chat_template(\n",
    "        test_messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(device)\n",
    "\n",
    "    from transformers import TextStreamer\n",
    "    print(\"ANSWER:\\n\")\n",
    "    text_streamer = TextStreamer(tokenizer_inference, skip_prompt=True)\n",
    "\n",
    "    _ = model_inference.generate(\n",
    "        input_ids=inputs,\n",
    "        streamer=text_streamer,\n",
    "        max_new_tokens=128,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.1,\n",
    "        eos_token_id=tokenizer_inference.eos_token_id,  # Esto evita tokens especiales raros\n",
    "        pad_token_id=tokenizer_inference.pad_token_id,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = model_inference.generate(\n",
    "    input_ids=inputs,\n",
    "    streamer=text_streamer,\n",
    "    max_new_tokens=128,\n",
    "    temperature=0.6,\n",
    "    top_p=0.3,\n",
    "    repetition_penalty=1.3,\n",
    "    eos_token_id=tokenizer_inference.eos_token_id,  # Esto evita tokens especiales raros\n",
    "    pad_token_id=tokenizer_inference.pad_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: later\n",
    "* Long Term memory\n",
    "* Coyote time in interaction\n",
    "* Filter: https://github.com/neurokitti/AIRIS-VtuberAI\n",
    "* Emotions\n",
    "* Character / Personality\n",
    "* Input centralizer\n",
    "* Avatar\n",
    "* Function calling\n",
    "* Gaming\n",
    "* stt -> OpenVoice AI\n",
    "* Monologue System for Inactivity: If there‚Äôs no new input from the audience, w-AI-fu triggers a monologue system that introduces new topics, preventing the conversation from going stale.\n",
    "* prompt with streaming system prompt [] and user: [vedal]: hey... So it can check names and separate from '[chat]\n",
    "* Need for Standardization ‚Äì He recognized the need for a standard interface, similar to PCI slots in PCs, to allow seamless integration of different AI models and tools. For diferent tools. And langchain usage?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mirai-uLPtLm8D-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
