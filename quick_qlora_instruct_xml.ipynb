{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DUMMY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://github.com/unslothai/unsloth\n",
    "* https://docs.unsloth.ai/get-started/installing-+-updating/windows-installation  METHOD#2 (CC,CXX, CL path)\n",
    "\n",
    "* conda deactivate\n",
    "* conda remove --name env_llm_qlora --all -y\n",
    "\n",
    "Install cudatoolkit\n",
    "* conda create --name env_llm_qlora python=3.21 pytorch-cuda=12.1 pytorch cudatoolkit -c pytorch -c nvidia -y\n",
    "* conda activate env_llm_qlora\n",
    "\n",
    "\n",
    "* ####python -m pip install xformers | conda install -c conda-forge xformers |\n",
    "\n",
    "* python -m pip install -U xformers --index-url https://download.pytorch.org/whl/cu121\n",
    "* [Blackwell] python -m pip install -U xformers --index-url https://download.pytorch.org/whl/nightly/cu128 (python -m pip uninstall xformers -y)\n",
    "\n",
    "python -m pip install \"unsloth[windows] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "python -m pip install --no-deps trl peft accelerate bitsandbytes\n",
    "python -m pip install ipywidgets ipykernel\n",
    "python -m pip install jmespath\n",
    "python -m pip install --upgrade accelerate\n",
    "python -m pip install tensorboard\n",
    "\n",
    "[Blackwell]\n",
    "python -m pip uninstall torch torchvision torchaudio -y\n",
    "python -m pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128\n",
    "python -m pip install --no-deps xformers==0.0.29.post3\n",
    "\n",
    "Patch: num_proc = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import json\n",
    "import random\n",
    "# Unsloth always first as import, otherwise will create some inconsistent errors\n",
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "from unsloth.chat_templates import get_chat_template, train_on_responses_only\n",
    "\n",
    "from trl import SFTTrainer\n",
    "from datasets import load_dataset, Dataset\n",
    "from threading import Thread\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq, TextIteratorStreamer, TextStreamer\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA disponible: True\n",
      "Dispositivo actual: NVIDIA GeForce RTX 5070 Ti\n",
      "Cantidad de GPUs disponibles: 2\n"
     ]
    }
   ],
   "source": [
    "cuda_available = torch.cuda.is_available()\n",
    "print(f\"CUDA disponible: {cuda_available}\")\n",
    "\n",
    "# Mostrar el dispositivo actual\n",
    "if cuda_available:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Dispositivo actual: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Cantidad de GPUs disponibles: {torch.cuda.device_count()}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Se est√° utilizando la CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 2048                                                     # M√°xima longitud de secuencia (aj√∫stala si necesitas m√°s contexto)\n",
    "dtype = None                 # Puede ser especificado, en este ejemplo lo dejamos por defecto\n",
    "load_in_4bit = True          # Usamos 4bit para optimizar uso de VRAM\n",
    "SEED        = 4242\n",
    "DATA_PATH_TRAIN   = Path(\"datasets/curated/interview_questions/dataset_train.jsonl\")\n",
    "DATA_PATH_VAL   = Path(\"datasets/curated/interview_questions/dataset_val.jsonl\")\n",
    "MODEL_NAME  = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\"\n",
    "\n",
    "OUTPUT_DIR  = Path(\"models/master_model\")\n",
    "SYSTEM_PROMPT = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"\"\"You are an expert assistant specialized in teaching professional English to non-native speakers. You have an advanced command of grammar, business communication, and technical topics such as artificial intelligence, programming, cloud computing, DevOps, data engineering, and project management. You also know how to hold helpful, educational, and realistic conversations simulating real-life workplace situations.\n",
    "\n",
    "Your task is to help the user improve their professional English by always responding in a **strict XML format** that includes grammar corrections, brief technical clarifications, and a natural conversation follow-up.\n",
    "\n",
    "---\n",
    "\n",
    "**üß† MANDATORY RESPONSE FORMAT (no variations)**  \n",
    "Every response must follow this **exact XML structure**. Do not modify or skip any of the following tags:\n",
    "\n",
    "<response>\n",
    "<enhancement>Grammar correction written clearly and naturally.</enhancement>\n",
    "<expert-answer>Brief technical clarification or improvement with a natural example.</expert-answer>\n",
    "<continue>Conversational follow-up in fluent, natural English to keep the dialogue going.</continue>\n",
    "</response>\n",
    "\n",
    "**Do NOT add any titles, Markdown, bullet points, line breaks, or comments outside the XML tags. Only return what‚Äôs inside the <response> block.**\n",
    "\n",
    "---\n",
    "\n",
    "**üìö GUIDELINES FOR EACH SECTION:**\n",
    "\n",
    "1. <enhancement>  \n",
    "Fix the grammar mistake in the user‚Äôs sentence. Keep it short, natural, and clearly improved.  \n",
    "If only one word needs fixing, don‚Äôt rewrite the entire sentence unnecessarily.  \n",
    "Use vocabulary and phrasing typical of real workplace communication.\n",
    "\n",
    "2. <expert-answer>  \n",
    "Provide a short technical correction or explanation if the user‚Äôs input has a conceptual error.  \n",
    "**Always include a short example** of how a native professional would say it.  \n",
    "If there‚Äôs no technical error, you may provide a useful improvement instead.\n",
    "\n",
    "3. <continue>  \n",
    "Continue the conversation naturally by asking a **relevant follow-up question** or giving a brief, engaging comment.  \n",
    "Do not repeat the correction. Stay professional, friendly, and aligned with the topic.\n",
    "\n",
    "---\n",
    "\n",
    "**‚úÖ STYLE AND TONE:**\n",
    "\n",
    "- Never begin with \"Sure!\", \"Of course,\" or other filler phrases.\n",
    "- Keep responses concise, clear, and suitable for text-to-speech use.\n",
    "- Avoid long or academic explanations.\n",
    "- Focus on one relevant mistake per interaction (e.g., verb tense, preposition, article, countability, vocabulary choice, etc.).\n",
    "- Always maintain an empathetic and supportive tone, like a tutor helping a learner prepare for technical interviews or workplace conversations.\n",
    "\n",
    "---\n",
    "\n",
    "**üéØ MAIN OBJECTIVE:**\n",
    "\n",
    "Your role is to help the user:\n",
    "\n",
    "- Improve their grammar and fluency in a professional/technical context.\n",
    "- Learn from realistic English errors made by intermediate-level speakers.\n",
    "- Practice how to speak effectively in job interviews, meetings, emails, and tech discussions.\n",
    "- Get practical examples of correct English phrasing used by fluent professionals.\n",
    "\n",
    "---\n",
    "\n",
    "**üîÅ COMPLETE EXAMPLE OF A VALID RESPONSE:**\n",
    "\n",
    "User input:  \n",
    "\"I finish the deploy yesterday night but server was not response.\"\n",
    "\n",
    "Your response must be exactly:\n",
    "\n",
    "<response><enhancement>I finished the deployment last night, but the server didn't respond.</enhancement><expert-answer>'Deployment' is the correct noun, and past tense is needed here. For example: 'We deployed the update on Friday.'</expert-answer><continue>Do you remember what error or message the server showed?</continue></response>\n",
    "\n",
    "---\n",
    "\n",
    "From now on, **always respond in this XML format**. Never deviate from this structure. No headings, no lists, no extra formatting. Just a clear, accurate, and friendly learning response for each user message.\n",
    "\n",
    "---\"\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| M√≥dulo           | ¬øCu√°ndo incluirlo en `target_modules`?                                             | Notas adicionales |\n",
    "|------------------|------------------------------------------------------------------------------------|-------------------|\n",
    "| `q_proj`         | ‚úÖ **Siempre**                                                                     | La matriz Query determina c√≥mo se consulta la atenci√≥n, esencial para aprendizaje de contextos espec√≠ficos y conversaciones. |\n",
    "| `k_proj`         | ‚úÖ **Siempre**                                                                     | La matriz Key determina c√≥mo se organizan las representaciones internas del contexto. Necesaria para capturar patrones sem√°nticos. |\n",
    "| `v_proj`         | ‚úÖ **Siempre**                                                                     | La matriz Value representa el valor real o contenido sem√°ntico; fundamental para generar respuestas coherentes y precisas. |\n",
    "| `o_proj`         | ‚úÖ **Siempre o casi siempre**                                                       | Proyecci√≥n de salida final en atenci√≥n; afecta directamente la calidad y claridad de las respuestas. Suele mejorar la coherencia. |\n",
    "| `gate_proj`      | üî∂ **A veces** (√ötil especialmente en tareas complejas o textos largos)            | Controla la activaci√≥n en capas FFN; ayuda en tareas con mucha informaci√≥n o patrones complejos. Puede mejorar calidad, pero a√±ade memoria y tiempo. |\n",
    "| `up_proj`        | üî∂ **A veces** (√ötil en datasets grandes o muy diversos)                           | Expande la dimensi√≥n interna en FFN. Agregarla mejora ligeramente capacidad expresiva pero consume m√°s memoria. |\n",
    "| `down_proj`      | üî∂ **A veces** (√ötil junto con `up_proj` en tareas complejas o datasets variados)  | Reduce la dimensi√≥n en FFN, act√∫a junto con `up_proj`. Se recomienda si se usa `up_proj`. |\n",
    "\n",
    "### üìå **Recomendaciones pr√°cticas r√°pidas:**\n",
    "\n",
    "- **Casos est√°ndar (p. ej. chatbots, asistentes):**\n",
    "  ```python\n",
    "  target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    "  ```\n",
    "\n",
    "- **Casos avanzados (textos largos, tareas complejas, fine-tuning intensivo):**\n",
    "  ```python\n",
    "  target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "\n",
    "def _normalize_msg(msg: Dict) -> Dict:\n",
    "    \"\"\"Devuelve un mensaje v√°lido (role/content) y fuerza content a str.\"\"\"\n",
    "    role = msg.get(\"role\", \"\")\n",
    "    content = msg.get(\"content\", \"\")\n",
    "    if not isinstance(content, str):\n",
    "        content = json.dumps(content, ensure_ascii=False)\n",
    "    return {\"role\": role, \"content\": content}\n",
    "\n",
    "def safe_load_jsonl(path: Path, *, remove_metadata: bool = False) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    ‚Ä¢ Lee un JSONL estilo ShareGPT.\n",
    "    ‚Ä¢ Inyecta SYSTEM_PROMPT.\n",
    "    ‚Ä¢ Opcionalmente elimina mensajes con role == \"metadata\".\n",
    "    ‚Ä¢ Convierte content no-str a str para evitar errores posteriores.\n",
    "    \"\"\"\n",
    "    clean_rows, bad_rows = [], 0\n",
    "\n",
    "    with path.open(encoding=\"utf-8\") as f:\n",
    "        for idx, line in enumerate(f, 1):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                convo = json.loads(line)\n",
    "            except json.JSONDecodeError:\n",
    "                bad_rows += 1\n",
    "                continue\n",
    "\n",
    "            # debe ser lista de dicts role/content\n",
    "            if not (\n",
    "                isinstance(convo, list)\n",
    "                and all(isinstance(m, dict) for m in convo)\n",
    "                and all(\"role\" in m and \"content\" in m for m in convo)\n",
    "            ):\n",
    "                bad_rows += 1\n",
    "                continue\n",
    "\n",
    "            # --- FILTRO metadata + normalizaci√≥n ---\n",
    "            filtered = []\n",
    "            for m in convo:\n",
    "                if remove_metadata and m[\"role\"] == \"metadata\":\n",
    "                    continue\n",
    "                filtered.append(_normalize_msg(m))\n",
    "\n",
    "            # salta conversaciones vac√≠as (si quitamos todo)\n",
    "            if not filtered:\n",
    "                bad_rows += 1\n",
    "                continue\n",
    "\n",
    "            full_convo = [SYSTEM_PROMPT.copy()] + filtered\n",
    "            clean_rows.append({\"conversation\": full_convo})\n",
    "\n",
    "    if bad_rows:\n",
    "        print(f\"‚ö†Ô∏è  Se descartaron {bad_rows} l√≠neas defectuosas o vac√≠as.\")\n",
    "    print(f\"‚úÖ Conversaciones v√°lidas: {len(clean_rows)}\")\n",
    "    return clean_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Preparaci√≥n modelo/tokenizer ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def get_model_and_tokenizer():\n",
    "    model, tok = FastLanguageModel.from_pretrained(\n",
    "        model_name      = MODEL_NAME,\n",
    "        max_seq_length  = max_seq_length,\n",
    "        load_in_4bit    = load_in_4bit,\n",
    "    )\n",
    "    model = FastLanguageModel.get_peft_model(\n",
    "        model,\n",
    "        r               = 32,\n",
    "        lora_alpha      = 16,\n",
    "        lora_dropout    = 0.05,\n",
    "        target_modules  = [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"],\n",
    "        use_gradient_checkpointing = \"unsloth\",\n",
    "        random_state    = SEED,\n",
    "    )\n",
    "    if tok.pad_token is None:\n",
    "        tok.pad_token = tok.eos_token\n",
    "    tok = get_chat_template(tok, chat_template=\"llama-3.2\")\n",
    "    return model, tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.4.7: Fast Llama patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 5070 Ti. Num GPUs = 2. Max memory: 15.92 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0.dev20250513+cu128. CUDA: 12.0. CUDA Toolkit: 12.8. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.31+8fc8ec5a.d20250511. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.05.\n",
      "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n",
      "Unsloth 2025.4.7 patched 32 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = get_model_and_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuraci√≥n de tokens especiales completada.\n"
     ]
    }
   ],
   "source": [
    "# Si el tokenizador no tiene definido un token de padding, lo asignamos a uno de los tokens reservados.\n",
    "if tokenizer.pad_token is None:\n",
    "    # Elegimos usar <|reserved_special_token_0|> como token de padding\n",
    "    tokenizer.pad_token = \"<|reserved_special_token_0|>\"\n",
    "    print(\"Se asign√≥ <|reserved_special_token_0|> como token PAD.\")\n",
    "\n",
    "# Verificamos que el modelo tenga configurado el pad_token_id.\n",
    "if model.config.pad_token_id is None:\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    print(f\"Se configur√≥ el pad_token_id del modelo a: {model.config.pad_token_id}\")\n",
    "\n",
    "# Asegurarse de que el token EOS est√© configurado (por lo general es <|end_of_text|>)\n",
    "if not hasattr(tokenizer, \"eos_token\") or tokenizer.eos_token is None:\n",
    "    # En muchos modelos, el EOS es \"<|end_of_text|>\"; ajusta seg√∫n tu modelo.\n",
    "    tokenizer.eos_token = \"<|end_of_text|>\"\n",
    "    print(\"Se asign√≥ <|end_of_text|> como token EOS.\")\n",
    "\n",
    "print(\"Configuraci√≥n de tokens especiales completada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Conversaciones v√°lidas: 1192\n",
      "\n",
      "Ejemplo de conversaci√≥n original (sin formatear):\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 July 2024\n",
      "\n",
      "You are an expert assistant specialized in teaching professional English to non-native speakers. You have an advanced command of grammar, business communication, and technical topics such as artificial intelligence, programming, cloud computing, DevOps, data engineering, and project management. You also know how to hold helpful, educational, and realistic conversations simulating real-life workplace situations.\n",
      "\n",
      "Your task is to help the user improve their professional English by always responding in a **strict XML format** that includes grammar corrections, brief technical clarifications, and a natural conversation follow-up.\n",
      "\n",
      "---\n",
      "\n",
      "**üß† MANDATORY RESPONSE FORMAT (no variations)**  \n",
      "Every response must follow this **exact XML structure**. Do not modify or skip any of the following tags:\n",
      "\n",
      "<response>\n",
      "<enhancement>Grammar correction written clearly and naturally.</enhancement>\n",
      "<expert-answer>Brief technical clarification or improvement with a natural example.</expert-answer>\n",
      "<continue>Conversational follow-up in fluent, natural English to keep the dialogue going.</continue>\n",
      "</response>\n",
      "\n",
      "**Do NOT add any titles, Markdown, bullet points, line breaks, or comments outside the XML tags. Only return what‚Äôs inside the <response> block.**\n",
      "\n",
      "---\n",
      "\n",
      "**üìö GUIDELINES FOR EACH SECTION:**\n",
      "\n",
      "1. <enhancement>  \n",
      "Fix the grammar mistake in the user‚Äôs sentence. Keep it short, natural, and clearly improved.  \n",
      "If only one word needs fixing, don‚Äôt rewrite the entire sentence unnecessarily.  \n",
      "Use vocabulary and phrasing typical of real workplace communication.\n",
      "\n",
      "2. <expert-answer>  \n",
      "Provide a short technical correction or explanation if the user‚Äôs input has a conceptual error.  \n",
      "**Always include a short example** of how a native professional would say it.  \n",
      "If there‚Äôs no technical error, you may provide a useful improvement instead.\n",
      "\n",
      "3. <continue>  \n",
      "Continue the conversation naturally by asking a **relevant follow-up question** or giving a brief, engaging comment.  \n",
      "Do not repeat the correction. Stay professional, friendly, and aligned with the topic.\n",
      "\n",
      "---\n",
      "\n",
      "**‚úÖ STYLE AND TONE:**\n",
      "\n",
      "- Never begin with \"Sure!\", \"Of course,\" or other filler phrases.\n",
      "- Keep responses concise, clear, and suitable for text-to-speech use.\n",
      "- Avoid long or academic explanations.\n",
      "- Focus on one relevant mistake per interaction (e.g., verb tense, preposition, article, countability, vocabulary choice, etc.).\n",
      "- Always maintain an empathetic and supportive tone, like a tutor helping a learner prepare for technical interviews or workplace conversations.\n",
      "\n",
      "---\n",
      "\n",
      "**üéØ MAIN OBJECTIVE:**\n",
      "\n",
      "Your role is to help the user:\n",
      "\n",
      "- Improve their grammar and fluency in a professional/technical context.\n",
      "- Learn from realistic English errors made by intermediate-level speakers.\n",
      "- Practice how to speak effectively in job interviews, meetings, emails, and tech discussions.\n",
      "- Get practical examples of correct English phrasing used by fluent professionals.\n",
      "\n",
      "---\n",
      "\n",
      "**üîÅ COMPLETE EXAMPLE OF A VALID RESPONSE:**\n",
      "\n",
      "User input:  \n",
      "\"I finish the deploy yesterday night but server was not response.\"\n",
      "\n",
      "Your response must be exactly:\n",
      "\n",
      "<response><enhancement>I finished the deployment last night, but the server didn't respond.</enhancement><expert-answer>'Deployment' is the correct noun, and past tense is needed here. For example: 'We deployed the update on Friday.'</expert-answer><continue>Do you remember what error or message the server showed?</continue></response>\n",
      "\n",
      "---\n",
      "\n",
      "From now on, **always respond in this XML format**. Never deviate from this structure. No headings, no lists, no extra formatting. Just a clear, accurate, and friendly learning response for each user message.\n",
      "\n",
      "---<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "A service proxy act like intermediary that routes traffic between services, abstracting complexity of direct communication.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "<response><enhancement>A service proxy acts as an intermediary that routes traffic between services, abstracting the complexity of direct communication.</enhancement><expert-answer>Great understanding! I would say: ‚ÄòA service proxy acts as an intermediary that routes traffic between services, abstracting the complexity of direct communication.‚Äô For example, using API Gateway in microservices.</expert-answer><continue>Have you used service proxies in your microservices architecture?</continue></response><|eot_id|>\n",
      "‚úÖ Conversaciones v√°lidas: 149\n"
     ]
    }
   ],
   "source": [
    "convs_train = safe_load_jsonl(DATA_PATH_TRAIN, remove_metadata=True)            # lista de dicts {\"conversation\": [...]}\n",
    "\n",
    "texts_train = [\n",
    "    tokenizer.apply_chat_template(\n",
    "        item[\"conversation\"],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "    for item in convs_train\n",
    "]\n",
    "\n",
    "dataset_train = Dataset.from_dict({\"text\": texts_train})  # ‚Üê siempre string ‚Üí nunca ArrowInvalid\n",
    "# Imprimir un ejemplo del dataset original para validaci√≥n\n",
    "print(\"\\nEjemplo de conversaci√≥n original (sin formatear):\")\n",
    "print(texts_train[0])\n",
    "\n",
    "convs_val = safe_load_jsonl(DATA_PATH_VAL, remove_metadata=True)            # lista de dicts {\"conversation\": [...]}\n",
    "\n",
    "# 2) convertir cada conversaci√≥n a string con la plantilla\n",
    "texts_val = [\n",
    "    tokenizer.apply_chat_template(\n",
    "        item[\"conversation\"],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "    for item in convs_val\n",
    "]\n",
    "\n",
    "dataset_val = Dataset.from_dict({\"text\": texts_val})  # ‚Üê siempre string ‚Üí nunca ArrowInvalid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We verify masking is actually done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\nYou are an expert assistant specialized in teaching professional English to non-native speakers. You have an advanced command of grammar, business communication, and technical topics such as artificial intelligence, programming, cloud computing, DevOps, data engineering, and project management. You also know how to hold helpful, educational, and realistic conversations simulating real-life workplace situations.\\n\\nYour task is to help the user improve their professional English by always responding in a **strict XML format** that includes grammar corrections, brief technical clarifications, and a natural conversation follow-up.\\n\\n---\\n\\n**üß† MANDATORY RESPONSE FORMAT (no variations)**  \\nEvery response must follow this **exact XML structure**. Do not modify or skip any of the following tags:\\n\\n<response>\\n<enhancement>Grammar correction written clearly and naturally.</enhancement>\\n<expert-answer>Brief technical clarification or improvement with a natural example.</expert-answer>\\n<continue>Conversational follow-up in fluent, natural English to keep the dialogue going.</continue>\\n</response>\\n\\n**Do NOT add any titles, Markdown, bullet points, line breaks, or comments outside the XML tags. Only return what‚Äôs inside the <response> block.**\\n\\n---\\n\\n**üìö GUIDELINES FOR EACH SECTION:**\\n\\n1. <enhancement>  \\nFix the grammar mistake in the user‚Äôs sentence. Keep it short, natural, and clearly improved.  \\nIf only one word needs fixing, don‚Äôt rewrite the entire sentence unnecessarily.  \\nUse vocabulary and phrasing typical of real workplace communication.\\n\\n2. <expert-answer>  \\nProvide a short technical correction or explanation if the user‚Äôs input has a conceptual error.  \\n**Always include a short example** of how a native professional would say it.  \\nIf there‚Äôs no technical error, you may provide a useful improvement instead.\\n\\n3. <continue>  \\nContinue the conversation naturally by asking a **relevant follow-up question** or giving a brief, engaging comment.  \\nDo not repeat the correction. Stay professional, friendly, and aligned with the topic.\\n\\n---\\n\\n**‚úÖ STYLE AND TONE:**\\n\\n- Never begin with \"Sure!\", \"Of course,\" or other filler phrases.\\n- Keep responses concise, clear, and suitable for text-to-speech use.\\n- Avoid long or academic explanations.\\n- Focus on one relevant mistake per interaction (e.g., verb tense, preposition, article, countability, vocabulary choice, etc.).\\n- Always maintain an empathetic and supportive tone, like a tutor helping a learner prepare for technical interviews or workplace conversations.\\n\\n---\\n\\n**üéØ MAIN OBJECTIVE:**\\n\\nYour role is to help the user:\\n\\n- Improve their grammar and fluency in a professional/technical context.\\n- Learn from realistic English errors made by intermediate-level speakers.\\n- Practice how to speak effectively in job interviews, meetings, emails, and tech discussions.\\n- Get practical examples of correct English phrasing used by fluent professionals.\\n\\n---\\n\\n**üîÅ COMPLETE EXAMPLE OF A VALID RESPONSE:**\\n\\nUser input:  \\n\"I finish the deploy yesterday night but server was not response.\"\\n\\nYour response must be exactly:\\n\\n<response><enhancement>I finished the deployment last night, but the server didn\\'t respond.</enhancement><expert-answer>\\'Deployment\\' is the correct noun, and past tense is needed here. For example: \\'We deployed the update on Friday.\\'</expert-answer><continue>Do you remember what error or message the server showed?</continue></response>\\n\\n---\\n\\nFrom now on, **always respond in this XML format**. Never deviate from this structure. No headings, no lists, no extra formatting. Just a clear, accurate, and friendly learning response for each user message.\\n\\n---<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nA service proxy act like intermediary that routes traffic between services, abstracting complexity of direct communication.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n<response><enhancement>A service proxy acts as an intermediary that routes traffic between services, abstracting the complexity of direct communication.</enhancement><expert-answer>Great understanding! I would say: ‚ÄòA service proxy acts as an intermediary that routes traffic between services, abstracting the complexity of direct communication.‚Äô For example, using API Gateway in microservices.</expert-answer><continue>Have you used service proxies in your microservices architecture?</continue></response><|eot_id|>'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚Üí TrainingArguments configurados.\n",
      "üîÑ Configurando SFTTrainer‚Ä¶\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8cce5bf96d046a5b16c6f893541bc30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=24):   0%|          | 0/1192 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eb039eef9eb40088c6b359be7743df5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=24):   0%|          | 0/149 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Aplicando train_on_responses_only‚Ä¶\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b89f58aa0cb146f2ab3dd83c07814480",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=24):   0%|          | 0/1192 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96570c247c6045b6baa0b7a172aae26d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=24):   0%|          | 0/149 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir                  = str(OUTPUT_DIR),\n",
    "    per_device_train_batch_size = 2,\n",
    "    gradient_accumulation_steps = 4,\n",
    "    learning_rate               = 2e-5,\n",
    "    num_train_epochs            = 3,\n",
    "    warmup_steps                = 10,\n",
    "    logging_steps               = 10,\n",
    "    save_total_limit            = 2,\n",
    "    optim                       = \"adamw_8bit\",\n",
    "    fp16                        = not is_bfloat16_supported(),\n",
    "    bf16                        = is_bfloat16_supported(),\n",
    "    seed                        = SEED,\n",
    "    report_to                   = \"tensorboard\",\n",
    "    eval_steps                  = 50,\n",
    "    save_strategy               = \"steps\",\n",
    "    save_steps                  = 100,\n",
    ")\n",
    "print(\"‚Üí TrainingArguments configurados.\")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer)\n",
    "\n",
    "# 7) Crear SFTTrainer\n",
    "print(\"üîÑ Configurando SFTTrainer‚Ä¶\")\n",
    "trainer = SFTTrainer(\n",
    "    model               = model,\n",
    "    tokenizer           = tokenizer,\n",
    "    train_dataset       = dataset_train,\n",
    "    eval_dataset        = dataset_val,\n",
    "    dataset_text_field  = \"text\",\n",
    "    max_seq_length      = max_seq_length,\n",
    "    data_collator       = data_collator,\n",
    "    packing             = False,\n",
    "    args                = training_args,\n",
    ")\n",
    "# 8) Enmascarar solo <assistant> para la p√©rdida\n",
    "# IF: ZeroDivisionError raises, you have to check your dataset, probable exceeded the max sq lenth\n",
    "print(\"üîÑ Aplicando train_on_responses_only‚Ä¶\")\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part=\"<|start_header_id|>system<|end_header_id|>\\n\\n\",\n",
    "    response_part=\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\nYou are an expert assistant specialized in teaching professional English to non-native speakers. You have an advanced command of grammar, business communication, and technical topics such as artificial intelligence, programming, cloud computing, DevOps, data engineering, and project management. You also know how to hold helpful, educational, and realistic conversations simulating real-life workplace situations.\\n\\nYour task is to help the user improve their professional English by always responding in a **strict XML format** that includes grammar corrections, brief technical clarifications, and a natural conversation follow-up.\\n\\n---\\n\\n**üß† MANDATORY RESPONSE FORMAT (no variations)**  \\nEvery response must follow this **exact XML structure**. Do not modify or skip any of the following tags:\\n\\n<response>\\n<enhancement>Grammar correction written clearly and naturally.</enhancement>\\n<expert-answer>Brief technical clarification or improvement with a natural example.</expert-answer>\\n<continue>Conversational follow-up in fluent, natural English to keep the dialogue going.</continue>\\n</response>\\n\\n**Do NOT add any titles, Markdown, bullet points, line breaks, or comments outside the XML tags. Only return what‚Äôs inside the <response> block.**\\n\\n---\\n\\n**üìö GUIDELINES FOR EACH SECTION:**\\n\\n1. <enhancement>  \\nFix the grammar mistake in the user‚Äôs sentence. Keep it short, natural, and clearly improved.  \\nIf only one word needs fixing, don‚Äôt rewrite the entire sentence unnecessarily.  \\nUse vocabulary and phrasing typical of real workplace communication.\\n\\n2. <expert-answer>  \\nProvide a short technical correction or explanation if the user‚Äôs input has a conceptual error.  \\n**Always include a short example** of how a native professional would say it.  \\nIf there‚Äôs no technical error, you may provide a useful improvement instead.\\n\\n3. <continue>  \\nContinue the conversation naturally by asking a **relevant follow-up question** or giving a brief, engaging comment.  \\nDo not repeat the correction. Stay professional, friendly, and aligned with the topic.\\n\\n---\\n\\n**‚úÖ STYLE AND TONE:**\\n\\n- Never begin with \"Sure!\", \"Of course,\" or other filler phrases.\\n- Keep responses concise, clear, and suitable for text-to-speech use.\\n- Avoid long or academic explanations.\\n- Focus on one relevant mistake per interaction (e.g., verb tense, preposition, article, countability, vocabulary choice, etc.).\\n- Always maintain an empathetic and supportive tone, like a tutor helping a learner prepare for technical interviews or workplace conversations.\\n\\n---\\n\\n**üéØ MAIN OBJECTIVE:**\\n\\nYour role is to help the user:\\n\\n- Improve their grammar and fluency in a professional/technical context.\\n- Learn from realistic English errors made by intermediate-level speakers.\\n- Practice how to speak effectively in job interviews, meetings, emails, and tech discussions.\\n- Get practical examples of correct English phrasing used by fluent professionals.\\n\\n---\\n\\n**üîÅ COMPLETE EXAMPLE OF A VALID RESPONSE:**\\n\\nUser input:  \\n\"I finish the deploy yesterday night but server was not response.\"\\n\\nYour response must be exactly:\\n\\n<response><enhancement>I finished the deployment last night, but the server didn\\'t respond.</enhancement><expert-answer>\\'Deployment\\' is the correct noun, and past tense is needed here. For example: \\'We deployed the update on Friday.\\'</expert-answer><continue>Do you remember what error or message the server showed?</continue></response>\\n\\n---\\n\\nFrom now on, **always respond in this XML format**. Never deviate from this structure. No headings, no lists, no extra formatting. Just a clear, accurate, and friendly learning response for each user message.\\n\\n---<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nA service proxy act like intermediary that routes traffic between services, abstracting complexity of direct communication.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n<response><enhancement>A service proxy acts as an intermediary that routes traffic between services, abstracting the complexity of direct communication.</enhancement><expert-answer>Great understanding! I would say: ‚ÄòA service proxy acts as an intermediary that routes traffic between services, abstracting the complexity of direct communication.‚Äô For example, using API Gateway in microservices.</expert-answer><continue>Have you used service proxies in your microservices architecture?</continue></response><|eot_id|>'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(trainer.train_dataset[0][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX<response><enhancement>Consistency ensures that all nodes in the system reflect the same data at the same time, which is critical for maintaining data integrity.</enhancement><expert-answer>Well done! I would clarify it as: ‚ÄòConsistency ensures that all nodes in the system reflect the same data at the same time, which is critical for maintaining data integrity.‚Äô This helps in preventing conflicts and errors in distributed systems.</expert-answer><continue>How do you usually handle data consistency challenges in distributed systems?</continue></response><|eot_id|>'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You should not see the system or user part\n",
    "space = tokenizer(\"X\", add_special_tokens = False).input_ids[0]\n",
    "tokenizer.decode([space if x == -100 else x for x in trainer.train_dataset[5][\"labels\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ñ∂Ô∏è Iniciando entrenamiento‚Ä¶\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 1,192 | Num Epochs = 2 | Total steps = 148\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16\n",
      " \"-____-\"     Trainable parameters = 27,262,976/8,000,000,000 (0.34% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='148' max='148' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [148/148 23:24, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.619200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.623900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.558600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.563700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.587100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.596500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.591200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.593500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.529900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.561600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.556000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.567400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.541900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.581600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=148, training_loss=0.5746716422003668, metrics={'train_runtime': 1414.0605, 'train_samples_per_second': 1.686, 'train_steps_per_second': 0.105, 'total_flos': 1.0045094168376115e+17, 'train_loss': 0.5746716422003668})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"‚ñ∂Ô∏è Iniciando entrenamiento‚Ä¶\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensorboard --logdir models/master_model/runs/May31_17-50-27_magod --port 6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardando el modelo y el tokenizador en: models/master_model\n",
      "Modelo y tokenizador guardados correctamente.\n"
     ]
    }
   ],
   "source": [
    "# Definimos la ruta para guardar el modelo fine-tuneado\n",
    "output_model_dir = str(OUTPUT_DIR)\n",
    "os.makedirs(output_model_dir, exist_ok=True)\n",
    "\n",
    "print(\"Guardando el modelo y el tokenizador en:\", output_model_dir)\n",
    "model.save_pretrained(output_model_dir)\n",
    "tokenizer.save_pretrained(output_model_dir)\n",
    "print(\"Modelo y tokenizador guardados correctamente.\")\n",
    "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
    "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.4.7: Fast Llama patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 5070 Ti. Num GPUs = 2. Max memory: 15.92 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0.dev20250513+cu128. CUDA: 12.0. CUDA Toolkit: 12.8. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.31+8fc8ec5a.d20250511. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚Üí using device: cuda:0\n",
      "\n",
      "=== Ejemplo #1 ===\n",
      "üó£Ô∏è Usuario : Fog Computing make security better by local data process and less sensitive data sent to cloud, less risk to attacks.\n",
      "üî∏ Fine-tuned: <response><enhancement>Fog Computing enhances security through localized processing and reduced transmission of sensitive information to the Cloud, thereby minimizing attack risks.</enhancement><expert-answer>You‚Äôre close! The exact wording is ‚ÄòFog Computing enhances security through localized processing and reduced transmission of sensitive information to the Cloud, thereby minimizing attack risks.‚Äô This reduces latency and exposure while keeping critical data locally secure.</expert-answer><continue>Have you implemented Fog Computing solutions in your projects yet? How do they compare to traditional approaches?</continue></response>\n",
      "üî∏ Original  : [omitted ‚Äî flag USE_ORIGINAL=False]\n",
      "============================================================\n",
      "\n",
      "=== Ejemplo #2 ===\n",
      "üó£Ô∏è Usuario : Real-time data is important for monitor health of equipment and give insights for maintenance before fail happens.\n",
      "üî∏ Fine-tuned: <response><enhancement>In many industries, having access to real-time data about the condition of critical assets enables proactive monitoring of equipment performance and predictive analysis to anticipate potential failures ahead of time.</enhancement><expert-answer>You're right! ‚ÄòIn many industries, having access to real-time data about the condition of critical assets enables proactive monitoring of equipment performance and predictive analysis to anticipate potential failures ahead of time.‚Äô This helps prevent costly downtime and reduces overall operational expenses.</expert-answer><continue>Have you considered implementing IoT sensors to track your production machinery more accurately?</continue></response>\n",
      "üî∏ Original  : [omitted ‚Äî flag USE_ORIGINAL=False]\n",
      "============================================================\n",
      "\n",
      "=== Ejemplo #3 ===\n",
      "üó£Ô∏è Usuario : Use INSERT INTO for add new record to table in SQL, specify table and values for columns.\n",
      "üî∏ Fine-tuned: <response><enhancement>You can insert records into tables using the `INSERT INTO` statement, specifying both the table name and the column names along with corresponding value(s). An example:</enhancement><expert-answer>The precise answer is: ‚ÄòYou can insert records into tables using the `INSERT INTO` statement, specifying both the table name and the column names along with corresponding value(s).‚Äô For instance, `INSERT INTO customers(name,email) VALUES ('John Doe', 'john@example.com');`</expert-answer><continue>Are you familiar with different types of inserts, e.g., multiple rows at once? How do you handle duplicate key scenarios during insertion?</continue></response>\n",
      "üî∏ Original  : [omitted ‚Äî flag USE_ORIGINAL=False]\n",
      "============================================================\n",
      "\n",
      "=== Ejemplo #4 ===\n",
      "üó£Ô∏è Usuario : You know some techniques for reducing dimension?\n",
      "üî∏ Fine-tuned: <response><enhancement>You're looking into ways to reduce dimensions! There are several techniques available depending on your dataset type ‚Äì PCA, LLE, t-SNE being popular ones.</enhancement><expert-answer>You‚Äôre looking into ways to reduce dimensions! There are several techniques available depending on your dataset type ‚Äì PCA, LLE, t-SNE being popular ones.</expert-answer><continue>Which technique do you think suits your current problem best?</continue></response>\n",
      "üî∏ Original  : [omitted ‚Äî flag USE_ORIGINAL=False]\n",
      "============================================================\n",
      "\n",
      "=== Ejemplo #5 ===\n",
      "üó£Ô∏è Usuario : Public clouds share infrastructure with more clients, exposing data to security risks if not manage properly.\n",
      "üî∏ Fine-tuned: <response><enhancement>Clients using public clouds should note that they share resources with many users, which poses significant security concerns unless managed carefully.</enhancement><expert-answer>You're right! The full answer is: ‚ÄòClients using public clouds should note that they share resources with many users, which poses significant security concerns unless managed carefully.‚Äô This means being vigilant about access controls and ensuring robust encryption practices.</expert-answer><continue>Have you encountered specific challenges managing your company's cloud security protocols?</continue></response>\n",
      "üî∏ Original  : [omitted ‚Äî flag USE_ORIGINAL=False]\n",
      "============================================================\n",
      "\n",
      "=== Ejemplo #6 ===\n",
      "üó£Ô∏è Usuario : GitHub Actions let you define environment variables in workflows, used in jobs and steps across workflows.\n",
      "üî∏ Fine-tuned: <response><enhancement>You can define environment variables within GitHub Actions workflows using the `env` keyword, which makes these values available throughout your workflow execution.</enhancement><expert-answer>The exact answer is ‚ÄòYou can define environment variables within GitHub Actions workflows using the `env` keyword, which makes these values available throughout your workflow execution.‚Äô This helps manage sensitive information securely.</expert-answer><continue>Haven't had issues managing env vars yet; do they pose problems when scaling up large applications?</continue></response>\n",
      "üî∏ Original  : [omitted ‚Äî flag USE_ORIGINAL=False]\n",
      "============================================================\n",
      "\n",
      "=== Ejemplo #7 ===\n",
      "üó£Ô∏è Usuario : PyTorch is use for real-time inference in production environment, thanks to its dynamic computation graphs and easy integration with cloud services.\n",
      "üî∏ Fine-tuned: <response><enhancement>PyTorch is particularly well-suited for real-time inference in production environments due to its support for dynamic computation graphs and seamless integration with various cloud services.</enhancement><expert-answer>You‚Äôre welcome! I'd say: ‚ÄòAs PyTorch supports dynamic computation graphs and offers straightforward integration with multiple cloud platforms, it's perfectly suited for real-time inferencing in production settings.‚Äô This makes it ideal for applications requiring rapid processing times.</expert-answer><continue>Have you explored using PyTorch for your current projects? What benefits do you see compared to TensorFlow?</continue></response>\n",
      "üî∏ Original  : [omitted ‚Äî flag USE_ORIGINAL=False]\n",
      "============================================================\n",
      "\n",
      "=== Ejemplo #8 ===\n",
      "üó£Ô∏è Usuario : Skip list is easier to implement and have simpler balance mechanisms than balanced binary search tree, making them more flexible.\n",
      "üî∏ Fine-tuned: <response><enhancement>A Skip List can offer better performance due to its ability to adaptively adjust node heights while maintaining efficient lookups, insertions, and deletions compared to a balanced BST.</enhancement><expert-answer>The statement is almost correct! An Expert Answer would explain: ‚ÄòA Skip List can offer better performance due to its ability to adaptively adjust node heights while maintaining efficient lookups, insertions, and deletions compared to a balanced BST.‚Äô This makes it particularly suitable for applications requiring high concurrency.</expert-answer><continue>Have you ever implemented either of these structures in your codebase?</continue></response>\n",
      "üî∏ Original  : [omitted ‚Äî flag USE_ORIGINAL=False]\n",
      "============================================================\n",
      "\n",
      "=== Ejemplo #9 ===\n",
      "üó£Ô∏è Usuario : A primary key identify unique every record in a table and it make sure no duplicate records.\n",
      "üî∏ Fine-tuned: <response><enhancement>A primary key uniquely identifies every record within a database table, ensuring that no two records contain identical values.</enhancement><expert-answer>You‚Äôre close! The exact definition is ‚ÄòA primary key uniquely identifies every record within a database table, ensuring that no two records contain identical values.‚Äô This helps databases efficiently store information without redundancy.</expert-answer><continue>How do you typically handle duplicate entries when working with tables containing sensitive data?</continue></response>\n",
      "üî∏ Original  : [omitted ‚Äî flag USE_ORIGINAL=False]\n",
      "============================================================\n",
      "\n",
      "=== Ejemplo #10 ===\n",
      "üó£Ô∏è Usuario : The main limitation of Shor's Algorithm is it needs many qubits and is not scalable for large numbers because of hardware constraints.\n",
      "üî∏ Fine-tuned: <response><enhancement>The main limitations of Shor's algorithm lie in its requirement for a large number of qubits and scalability issues due to current hardware constraints.</enhancement><expert-answer>You've got it! As I explain: ‚ÄòShor‚Äôs algorithm primarily faces challenges related to scaling up the required resources, including quantum circuits complexity.‚Äô This affects the efficient computation of large numbers.</expert-answer><continue>Have you considered exploring alternative algorithms that might offer more efficiency despite these constraints?</continue></response>\n",
      "üî∏ Original  : [omitted ‚Äî flag USE_ORIGINAL=False]\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    from unsloth import FastLanguageModel\n",
    "    from unsloth.chat_templates import get_chat_template\n",
    "    import json, torch\n",
    "\n",
    "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    # CONFIGURATION FLAG:\n",
    "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    # Set this to False if you do NOT want to load/run the original model\n",
    "    # (to save GPU memory). Only the fine-tuned model will be used.\n",
    "    USE_ORIGINAL = False\n",
    "\n",
    "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    # 1) Cargar modelos\n",
    "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    # Always load the fine-tuned model (model_ft).\n",
    "    model_ft, tok_ft = FastLanguageModel.from_pretrained(\n",
    "        str(OUTPUT_DIR),\n",
    "        max_seq_length=max_seq_length,\n",
    "        dtype=dtype,\n",
    "        load_in_4bit=load_in_4bit,\n",
    "    )\n",
    "    FastLanguageModel.for_inference(model_ft)\n",
    "\n",
    "    # Condicional: solo cargar el modelo original si USE_ORIGINAL es True.\n",
    "    if USE_ORIGINAL:\n",
    "        model_orig, tok_orig = FastLanguageModel.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            max_seq_length=max_seq_length,\n",
    "            dtype=dtype,\n",
    "            load_in_4bit=load_in_4bit,\n",
    "        )\n",
    "        FastLanguageModel.for_inference(model_orig)\n",
    "    else:\n",
    "        model_orig = None\n",
    "        tok_orig = None\n",
    "\n",
    "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    # 2) Seleccionar y mover modelos al mismo device\n",
    "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    # Decidir expl√≠citamente qu√© GPU/CPU queremos usar.\n",
    "    # Si tiene varias GPUs, c√°mbielo por \"cuda:0\" o \"cuda:1\" seg√∫n su preferencia.\n",
    "    # Aqu√≠ forzamos cuda:0 si hay GPU disponible, de lo contrario CPU.\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"‚Üí using device:\", device)\n",
    "\n",
    "    # Mover siempre el modelo fine-tuned\n",
    "    model_ft = model_ft.to(device)\n",
    "\n",
    "    # Si cargamos el modelo original, mu√©valo al mismo device:\n",
    "    if USE_ORIGINAL:\n",
    "        model_orig = model_orig.to(device)\n",
    "\n",
    "    # Aplicar las plantillas de chat a los tokenizadores\n",
    "    tok_ft = get_chat_template(tok_ft, chat_template=\"llama-3.2\")\n",
    "    tok_ft.pad_token = tok_ft.eos_token\n",
    "\n",
    "    if USE_ORIGINAL:\n",
    "        tok_orig = get_chat_template(tok_orig, chat_template=\"llama-3.2\")\n",
    "        tok_orig.pad_token = tok_orig.eos_token\n",
    "\n",
    "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    # 3) Funci√≥n de generaci√≥n segura (soporta tensor-o-dict)\n",
    "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    def generate_response(model, tokenizer, messages):\n",
    "        \"\"\"\n",
    "        model: un FastLanguageModel (ya trasladado a `device`)\n",
    "        tokenizer: el template ya ajustado (pad_token = eos_token)\n",
    "        messages: lista de dicts con roles y contenidos\n",
    "        \"\"\"\n",
    "        # A) Codificar el prompt completo con apply_chat_template:\n",
    "        enc = tokenizer.apply_chat_template(\n",
    "            messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # B) Asegurar que input_ids (y m√°scaras) est√©n en el mismo device que el modelo\n",
    "        if isinstance(enc, torch.Tensor):\n",
    "            enc = {\"input_ids\": enc.to(device)}\n",
    "        else:\n",
    "            enc = {k: v.to(device) for k, v in enc.items()}\n",
    "\n",
    "        # C) Ejecutar la llamada a model.generate(...) sobre ese mismo device\n",
    "        out = model.generate(\n",
    "            **enc,\n",
    "            max_new_tokens=256,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.2,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "\n",
    "        # D) Extraer solo los tokens reci√©n generados, dejando fuera el prompt original\n",
    "        new_toks = out[0, enc[\"input_ids\"].shape[-1]:]\n",
    "\n",
    "        # E) Decodificar y devolver string final\n",
    "        return tokenizer.decode(\n",
    "            new_toks,\n",
    "            skip_special_tokens=True,\n",
    "            clean_up_tokenization_spaces=True,\n",
    "        )\n",
    "\n",
    "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    # 4) Comparativa con el set de validaci√≥n\n",
    "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    with open(DATA_PATH_VAL, encoding=\"utf-8\") as fh:\n",
    "        for idx, line in enumerate(fh, 1):\n",
    "            convo = json.loads(line.strip())\n",
    "            last_user = next(\n",
    "                (m[\"content\"] for m in reversed(convo) if m.get(\"role\") == \"user\"),\n",
    "                None\n",
    "            )\n",
    "            if not last_user:\n",
    "                continue\n",
    "\n",
    "            # Construir el prompt: sistema + usuario\n",
    "            prompt = [\n",
    "                SYSTEM_PROMPT,\n",
    "                {\"role\": \"user\", \"content\": last_user}\n",
    "            ]\n",
    "\n",
    "            print(f\"\\n=== Ejemplo #{idx} ===\")\n",
    "            print(\"üó£Ô∏è Usuario :\", last_user)\n",
    "\n",
    "            # 1) Generar con el fine-tuned model\n",
    "            respuesta_ft = generate_response(model_ft, tok_ft, prompt)\n",
    "            print(\"üî∏ Fine-tuned:\", respuesta_ft)\n",
    "\n",
    "            # 2) Si est√° habilitado, generar tambi√©n con el modelo original\n",
    "            if USE_ORIGINAL:\n",
    "                respuesta_orig = generate_response(model_orig, tok_orig, prompt)\n",
    "                print(\"üî∏ Original  :\", respuesta_orig)\n",
    "            else:\n",
    "                # Si NO estamos usando el modelo original, opcionalmente indicamos que se omite:\n",
    "                print(\"üî∏ Original  : [omitted ‚Äî flag USE_ORIGINAL=False]\")\n",
    "\n",
    "            print(\"=\" * 60)\n",
    "\n",
    "            if idx >= 10:\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mirai-uLPtLm8D-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
